{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Environment\n",
    "First, we'll create an environment class that simulates the SIS epidemic dynamics, including the arrival and departure processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SISEpidemicEnv:\n",
    "    def __init__(self, n_susceptible, n_infected, lambda_arr, lambda_dep, infection_rate, recovery_rate):\n",
    "        self.initial_state = (n_susceptible, n_infected, lambda_arr, lambda_dep)\n",
    "        self.infection_rate = infection_rate\n",
    "        self.recovery_rate = recovery_rate\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        n_susceptible, n_infected, lambda_arr, lambda_dep = self.state\n",
    "        \n",
    "        # Simulate arrivals and departures\n",
    "        arrivals = np.random.poisson(lambda_arr)\n",
    "        departures = np.random.poisson(lambda_dep)\n",
    "        \n",
    "        # Update susceptible and infected counts\n",
    "        new_infected = np.random.binomial(n_susceptible, 1 - np.exp(-self.infection_rate * n_infected))\n",
    "        new_recovered = np.random.binomial(n_infected, 1 - np.exp(-self.recovery_rate))\n",
    "        \n",
    "        n_susceptible = max(n_susceptible + arrivals - departures - new_infected, 0)\n",
    "        n_infected = max(n_infected + new_infected - new_recovered, 0)\n",
    "        \n",
    "        # Update state\n",
    "        self.state = (n_susceptible, n_infected, lambda_arr, lambda_dep)\n",
    "        \n",
    "        # Calculate reward (negative of the number of infected individuals)\n",
    "        reward = -n_infected\n",
    "        \n",
    "        return self.state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructor (__init__): Initializes the environment with the initial number of susceptible (n_susceptible) and infected (n_infected) individuals, arrival (lambda_arr) and departure (lambda_dep) rates, infection rate (infection_rate), and recovery rate (recovery_rate).\n",
    "Reset: Resets the environment to the initial state and returns it.\n",
    "Step: Simulates one time step in the environment. This involves:\n",
    "Simulating arrivals and departures using Poisson processes.\n",
    "Updating the number of susceptible and infected individuals based on the infection and recovery processes.\n",
    "Returning the new state and the reward (negative of the number of infected individuals).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Q-Learning Algorithm\n",
    "Next, we'll implement the Q-learning algorithm to learn the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m     49\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 50\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m agent\u001b[38;5;241m.\u001b[39mlearn(state, action, reward, next_state)\n\u001b[1;32m     54\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[0;34m(state, state_bins)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscretize_state\u001b[39m(state, state_bins):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_bins\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscretize_state\u001b[39m(state, state_bins):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s, bins \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(state, state_bins))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdigitize\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:5559\u001b[0m, in \u001b[0;36mdigitize\u001b[0;34m(x, bins, right)\u001b[0m\n\u001b[1;32m   5557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[1;32m   5558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1387\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1385\u001b[0m \n\u001b[1;32m   1386\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_bins, action_space, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
    "        self.state_bins = state_bins\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = np.zeros(tuple(len(bins) + 1 for bins in state_bins) + (action_space,))\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.choice(self.action_space)\n",
    "        return np.argmin(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmin(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        td_error = td_target - self.q_table[state + (action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * td_error\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        \n",
    "def discretize_state(state, state_bins):\n",
    "    return tuple(np.digitize(s, bins) - 1 for s, bins in zip(state, state_bins))\n",
    "\n",
    "# Define state and action space bins\n",
    "n_susceptible_bins = np.linspace(0, 100, 10)\n",
    "n_infected_bins = np.linspace(0, 50, 10)\n",
    "lambda_arr_bins = np.linspace(0, 0, 5)\n",
    "lambda_dep_bins = np.linspace(0, 0, 5)\n",
    "state_bins = [n_susceptible_bins, n_infected_bins, lambda_arr_bins, lambda_dep_bins]\n",
    "\n",
    "action_space = 3  # Example: 0: No action, 1: Quarantine, 2: Vaccination\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = SISEpidemicEnv(100, 10, 2, 1, 0.05, 0.01)\n",
    "agent = QLearningAgent(state_bins, action_space)\n",
    "\n",
    "# Training\n",
    "n_episodes = 1000\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = discretize_state(state, state_bins)\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        next_state = discretize_state(next_state, state_bins)\n",
    "        \n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        if reward == 0:  # termination condition: no infected individuals? Is this the reason why it is non  convergent?\n",
    "            done = True # new terimnaitoni condition for suboptimality, less than 5 or 3% of the population is infected (termaiton at time before )\n",
    "\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructor (__init__): Initializes the Q-learning agent with state bins, action space, learning rate, discount factor, exploration rate, and exploration decay. It also initializes the Q-table.\n",
    "Choose Action: Chooses an action based on an epsilon-greedy strategy (random action with probability exploration_rate, otherwise the action with the minimum Q-value).\n",
    "Learn: Updates the Q-table using the Q-learning update rule:\n",
    "Calculates the TD target and TD error.\n",
    "Updates the Q-value for the current state-action pair.\n",
    "Decays the exploration rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Learned Policy\n",
    "\n",
    "Finally, we evaluate the learned policy by running the environment with the learned Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m     next_state, reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     10\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m discretize_state(next_state, state_bins)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "state = env.reset()\n",
    "state = discretize_state(state, state_bins)\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmin(agent.q_table[state])\n",
    "    next_state, reward = env.step(action)\n",
    "    next_state = discretize_state(next_state, state_bins)\n",
    "    \n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if reward == 0:  # Example termination condition: no infected individuals\n",
    "        done = True\n",
    "\n",
    "print(\"Evaluation completed. Total reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "\n",
    "# Plot optimal control policy for different initial states\n",
    "optimal_actions = np.zeros((len(n_susceptible_bins), len(n_infected_bins)))\n",
    "\n",
    "for i, n_susceptible in enumerate(n_susceptible_bins):\n",
    "    for j, n_infected in enumerate(n_infected_bins):\n",
    "        state = (n_susceptible, n_infected, 2, 1)  # Fixing lambda_arr and lambda_dep for visualization\n",
    "        discretized_state = discretize_state(state, state_bins)\n",
    "        optimal_action = np.argmin(agent.q_table[discretized_state])\n",
    "        optimal_actions[i, j] = optimal_action\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(optimal_actions, cmap='viridis', origin='lower')\n",
    "plt.colorbar(ticks=[0, 1, 2])\n",
    "plt.xticks(range(len(n_infected_bins)), np.round(n_infected_bins, 2), rotation=90)\n",
    "plt.yticks(range(len(n_susceptible_bins)), np.round(n_susceptible_bins, 2))\n",
    "plt.xlabel('Number of Infected')\n",
    "plt.ylabel('Number of Susceptible')\n",
    "plt.title('Optimal Actions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "state = env.reset()\n",
    "state = discretize_state(state, state_bins)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state_trajectory = [state]\n",
    "\n",
    "while not done:\n",
    "    action = np.argmin(agent.q_table[state])\n",
    "    next_state, reward = env.step(action)\n",
    "    next_state = discretize_state(next_state, state_bins)\n",
    "    \n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    state_trajectory.append(state)\n",
    "    if reward == 0:  # Example termination condition: no infected individuals\n",
    "        done = True\n",
    "\n",
    "print(\"Evaluation completed. Total reward:\", total_reward)\n",
    "\n",
    "# Plot state trajectory during evaluation\n",
    "state_trajectory = np.array(state_trajectory)\n",
    "plt.figure()\n",
    "plt.plot(state_trajectory[:, 0], label='Susceptible')\n",
    "plt.plot(state_trajectory[:, 1], label='Infected')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Count')\n",
    "plt.title('State Trajectory during Evaluation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
